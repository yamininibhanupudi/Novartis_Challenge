{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict if server will be hacked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "from pandas_profiling import ProfileReport\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as seab\n",
    "\n",
    "# importing libraries for preprocessing\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# importing the classifiers and metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost  import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold \n",
    "from datetime import datetime \n",
    "\n",
    "print('Successfully imported libraries !!!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train and test csv files\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)   #to display 100 charactors in each column\n",
    "train_data = pd.read_csv(r'D:\\jobs\\A! Hackathon\\Novartis Challenge\\Dataset\\train.csv')\n",
    "test_data = pd.read_csv(r'D:\\jobs\\A! Hackathon\\Novartis Challenge\\Dataset\\test.csv')\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "The first step for creating an ML model is exploring the given datasets (train.csv and test.csv). This is a crucial step.\n",
    "\n",
    "To explore the data I had generated a report with pandas_profiling. It generates profile reports from a pandas DataFrame. The pandas df.describe() function is great but a little basic for serious exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis.\n",
    "\n",
    "For each column the following statistics - if relevant for the column type - are presented in an interactive HTML report:\n",
    "\n",
    "- Type inference: detect the types of columns in a dataframe.\n",
    "- Essentials: type, unique values, missing values\n",
    "- Quantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range\n",
    "- Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation,   kurtosis, skewness\n",
    "- Most frequent values\n",
    "- Histogram\n",
    "- Correlations highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices\n",
    "- Missing values matrix, count, heatmap and dendrogram of missing values\n",
    "- Text analysis learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas profilling is a html widget. Hence, it would not be accessible when the notebook were downloaded. Therefore, to support my inferences I will be showing some plots and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the histograms of all the variables\n",
    "\n",
    "train_data.hist(bins=10, figsize=(14,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes based on data_profilling\n",
    "- Highly imbalanced target variable (Multiple_offense). There are almost 20 times more positive examples than negative ones\n",
    "- Variables X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10m X_11, x_12, X_13, X_14 and X_15 are the anonymized logging parameters which were provided and will be used as features\n",
    "- Among these X_10 and X_12 are highly skewed where as X_2 and X_3 are highly correlated with each other\n",
    "- Missing values are <0.1% (all from X_12). These values would be repaced with zeros\n",
    "- ID and Date fields are highly cardinal so we can drop them from both train and test sets\n",
    "\n",
    "In order to build a model for this classification model, my approach would be as follows.\n",
    "\n",
    "1. Data-Preprocessing\n",
    "   1.1     Filling in missing values\n",
    "   1.2     SMOTE for imbalanced data\n",
    "   1.3     Scaling the features\n",
    "2. Model Comparisions and validation\n",
    "   2.1     Testing different models (primarily ensembles and trees models\n",
    "   2.2     Compare based on accuracy as well as execution time and choose best model \n",
    "3. Checking feature importance and dropping irrelevant features\n",
    "4. Predict on test set to get our final predictions\n",
    "5. Submit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping high cardinal variable and removing duplicates (from both train and test datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicates, also INCIDENT_ID and DATE columns from the train/test dataset\n",
    "\n",
    "train_data = train_data.drop(['INCIDENT_ID', 'DATE'], axis=1)\n",
    "train_data.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "# print('train_data shape: {}'.format(train_data.shape))\n",
    "\n",
    "index = pd.DataFrame()\n",
    "index['INCIDENT_ID'] = test_data['INCIDENT_ID']\n",
    "# print('index shape: {}'.format(index.shape))\n",
    "\n",
    "test_data = test_data.drop(['INCIDENT_ID','DATE'], axis=1)\n",
    "# print('test_data shape: {}'.format(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data['MULTIPLE_OFFENSE']\n",
    "train_data = train_data.drop(['MULTIPLE_OFFENSE'], axis=1)\n",
    "\n",
    "# print('Target shape: {}'.format(y.shape))\n",
    "# print('train_data shape: {}'.format(train_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing values (with 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()   # we find that there are 182 missing values in variable x_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info()   # we find that there are 127 missing values in variable x_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that there are 182 and 127  missing values in train and test datasets, respectively, for the variable x_12. \n",
    "# Let's fill these missing values with 0\n",
    "\n",
    "train_data.fillna(0, inplace=True)\n",
    "test_data.fillna(0, inplace=True)\n",
    "# print(train_data.isnull().sum())   # to check if successful --- Done!\n",
    "# print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling imbalanced data (using SMOTE) ---- try undersampling next \n",
    "\n",
    "source: https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying smote to balance the imbalanced labels\n",
    "\n",
    "print(\"Number records train_data dataset: \", train_data.shape)\n",
    "print(\"Number records test_data dataset: \", test_data.shape)\n",
    "print(\"\\nNumber labels: \", y.shape)\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y==1)))    \n",
    "print(\"Before OverSampling, counts of label '0': {}\".format(sum(y==0)))\n",
    "\n",
    "oversample = SMOTE()\n",
    "X_train, Y = oversample.fit_resample(train_data, y)\n",
    "\n",
    "print('\\nAfter OverSampling, the shape of X_train: {}'.format(X_train.shape))\n",
    "print('After OverSampling, the shape of Y: {}'.format(Y.shape))\n",
    "\n",
    "print(\"\\nAfter OverSampling, counts of label '1': {}\".format(sum(Y==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(Y==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling features\n",
    "\n",
    "The effect of different scalers (namely, StandardScaler(), MinMaxScaler() and PowerTransformer()) were compared before the PowerTransformer was used to remove the mean and scale the data to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "scaler = PowerTransformer(method='yeo-johnson')\n",
    "scaler.fit(train_data)\n",
    "X_features = pd.DataFrame(scaler.transform(X_train), columns = X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(test_data), columns = test_data.columns)\n",
    "\n",
    "# print(X_features.shape)\n",
    "# print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features.hist(bins=10, figsize=(14,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the above plots to the non-scaled plots, we can see that all the features have been centred at zero after scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Comparision and Validation\n",
    "\n",
    "Now that we are done with data preprocessing, let's move on to make some models. The models that will be trained are:\n",
    "1. Logistic Regression Classifier\n",
    "2. Random Forests Classifier\n",
    "3. Gradient Boosted Trees Classifier\n",
    "4. Decision Trees Classifier\n",
    "5. Extra Treese Classifier \n",
    "6. LightGBM Classifier\n",
    "7. XGBoost Classifier\n",
    "8. CatBoost Classifier\n",
    "9. KNN Classifier\n",
    "\n",
    "The ensemble and trees models were used because they can handle class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "\n",
    "GridSearchCV was performed on the chosen models to get the best possible parameters. These can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# training Logistic regression\n",
    "lr = LogisticRegression()\n",
    "params = { 'class_weight':['balanced', None],\n",
    "           'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "           'C' : np.logspace(-3,3,7), \n",
    "           'penalty' : [\"l1\",\"l2\"],\n",
    "          \n",
    "         }\n",
    "gs = GridSearchCV(lr, params, cv=5, scoring='recall', verbose=1, n_jobs=-1)\n",
    "gs.fit(X_features, Y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "-------------------------------------------------------------------------------------\n",
    "# Training Random Forest Classifier\n",
    "rf = RandomForestClassifier()\n",
    "params = {'criterion' : ['gini', 'entropy'],\n",
    "          'n_estimators' : [10, 50, 100, 500, 1000], \n",
    "          'max_depth' : [7, 11, 15, 19]\n",
    "         }\n",
    "gs = GridSearchCV(rf, params, cv=5, scoring='recall', verbose = 1, n_jobs = -1)\n",
    "gs.fit(X_features, Y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "---------------------------------------------------------------------------------------\n",
    "# Training Gradient Boosted Trees\n",
    "gb = GradientBoostingClassifier()\n",
    "params = {'n_estimators' : [10, 50, 100, 500, 1000], \n",
    "          'max_depth' : [7, 11, 15, 19, 25, 30]\n",
    "         }\n",
    "gs = GridSearchCV(gb, params, cv=5, scoring='recall', verbose = 1, n_jobs = -1)\n",
    "gs.fit(X_features, Y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    "# Training Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "params = {'max_depth' : [7, 11, 15, 19, 25, 30, 60, 90], \n",
    "          'max_features' : [1000, 2000, 5000, 10000]\n",
    "         }\n",
    "gs = GridSearchCV(dt, params, cv=5, scoring='recall', verbose = 1, n_jobs = -1)\n",
    "gs.fit(X_features, Y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    "# Training Extra trees classifiers\n",
    "etc = ExtraTreesClassifier(random_state=10)\n",
    "params = {'n_estimators' : [10, 50, 100, 500, 1000], \n",
    "          'max_features' : [5, 10, 50, 100]\n",
    "         }\n",
    "gs = GridSearchCV(etc, params, cv=5, scoring='recall', verbose = 1, n_jobs = -1)\n",
    "gs.fit(X_features, Y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    "# Training LightGBM ---- https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm = LGBMClassifier()\n",
    "params = { 'learning_rate': [0.005, 0.001, 0.01, 0.05, 0.1],\n",
    "           'n_estimators': [5, 10, 50, 100, 500, 1000],\n",
    "           'max_depth' : [7, 11, 15, 19],       # helps control overfitting\n",
    "           'num_leaves': [30, 60, 90],          # correlated with max_depth, double the max_depth gives high accuracy\n",
    "           'bagging_fraction' : [0.5],  # to speed up\n",
    "           'bagging_freq' : [5],        # to speed up\n",
    "           'feature_fraction' : [0.5],  # to speed up\n",
    "           'boosting_type' : ['gbdt'],\n",
    "           'objective' : ['binary'],\n",
    "           'reg_alpha': [0, 0.01, 0.1, 1, 2, 5, 7, 10, 50, 100],\n",
    "           'reg_lambda': [0, 0.01, 0.1, 1, 5, 10, 20, 50, 100]\n",
    "         }\n",
    "gs = GridSearchCV(lgbm, params, cv=5, scoring='recall', verbose = 1, n_jobs = -1)\n",
    "gs.fit(X_features, Y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "----------------------------------------------------------------------------------------------\n",
    "# Training xgb Classifier\n",
    "xgb = XGBClassifier()\n",
    "params = {'n_estimators': [50, 100, 500, 1000],\n",
    "          'max_depth': [7, 9, 15, 19],    # typical values 3-10, higher values cause overfitting\n",
    "          'learning_rate': [0.05, 0.01, 0.5, 0.1],\n",
    "          'gamma': [0, 1, 5],\n",
    "          'min_child_weight' : [1, 5, 10],   # to avoid overfitting, too high values cause underfitting\n",
    "          'subsample' : [0.5, 0.8, 1],    #lower values make it conservative, too low values lead to underfitting\n",
    "          'colsample_bytree' : [0.5, 0.8, 1]    #same as max_features in GBT\n",
    "         }\n",
    "gs = GridSearchCV(xgb, params, cv=5, scoring='recall', verbose = 1, n_jobs = -1)\n",
    "gs.fit(X_features, Y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "------------------------------------------------------------------------------------------------\n",
    "# Catclassifier\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cbt = CatBoostClassifier()\n",
    "params = {'depth' : [7, 11, 15, 19],\n",
    "          'learning_rate' : [0.005, 0.001, 0.01, 0.05, 0.1],   # how fast can reach the global/local optima\n",
    "          'iterations'    : [30, 50, 100],    # many iterations would mean using more trees to fit --- risk of overfitting\n",
    "          'l2_leaf_reg': [1, 3, 5, 7, 9],   #l2 reguralization term in cost funtion\n",
    "          'border_count':[5, 10, 20, 30, 50, 100, 200, 254],   #254 is usually used for best possible results on CPU or GPU---this parameter impacts speed\n",
    "          'bagging_temperature' : [0, 1]    # 0 means equal weight assigned to all features, 1 means weighted\n",
    "         }\n",
    "gs = GridSearchCV(cbt, params, cv=5, scoring='recall', verbose = 1, n_jobs = -1)\n",
    "gs.fit(X_features, Y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "-------------------------------------------------------------------------------------------------\n",
    "# knn classifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "k_range = list(range(1,31))\n",
    "weight_options = [\"uniform\", \"distance\"]\n",
    "params = dict(n_neighbors = k_range, weights = weight_options)\n",
    "\n",
    "gs = GridSearchCV(knn, params, cv=5, scoring='recall', verbose = 1, n_jobs = -1)\n",
    "gs.fit(X_features, Y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add our tuned models into list\n",
    "models = []\n",
    "\n",
    "lr_model = LogisticRegression(C=0.01, penalty='l1', solver='liblinear', random_state = 10)\n",
    "\n",
    "rf_model = RandomForestClassifier(criterion='entropy', max_depth=19, n_estimators=100, random_state = 10)\n",
    "\n",
    "gb_model = GradientBoostingClassifier(max_depth=7, n_estimators=500, random_state = 10)\n",
    "\n",
    "dt_model = DecisionTreeClassifier(max_depth=15, max_features=15, random_state = 10)\n",
    "\n",
    "etc_model = ExtraTreesClassifier(n_estimators= 500, max_features=15, random_state = 10)\n",
    "\n",
    "lgbm_model = LGBMClassifier(learning_rate= 0.03, n_estimators= 500, max_depth= 7, num_leaves= 50, bagging_fraction= 0.5, \n",
    "                            bagging_freq= 2, feature_fraction= 0.7, boosting_type= 'gbdt', \n",
    "                            objective= 'binary', reg_alpha= 0, reg_lambda= 0, random_state = 10)\n",
    "\n",
    "xgb_model = XGBClassifier(colsample_bytree=0.5, gamma=0, learning_rate=0.01, \n",
    "                          max_depth=15, min_child_weight=1, n_estimators=1000, subsample=1, random_state = 10)\n",
    "\n",
    "cbt_model = CatBoostClassifier(depth=16, learning_rate=0.01, iterations=15, border_count=254, \n",
    "                               bagging_temperature=0, silent=True, random_state = 10)\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "\n",
    "models.append(('Logistic Regression', lr_model))\n",
    "models.append(('Random Forest', rf_model))\n",
    "models.append(('Gradient Boosting Trees', gb_model))\n",
    "models.append(('Decision Trees', dt_model))\n",
    "models.append(('Extra Trees', etc_model))\n",
    "models.append(('LightGBM', lgbm_model))\n",
    "models.append(('XGBoost', xgb_model))\n",
    "models.append(('CatBoost', cbt_model))\n",
    "models.append(('K Nearest Neighbours Classifier', knn_model))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "# evaluate each model in turn\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, shuffle = True, random_state = 10)\n",
    "    cv_results = model_selection.cross_val_score(model, X_features, Y, cv = 5, scoring = 'recall')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    # print mean accuracy and standard deviation\n",
    "    print(\"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 10))\n",
    "plt.boxplot(results)\n",
    "plt.title('Algorithm Comparison')\n",
    "plt.xticks([1,2,3,4,5,6,7,8,9], names, rotation=45, horizontalalignment='right', fontweight='light')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the obove we can see that XGBoost, LightGBM and Gradient Boosting Trees perform the best among all the classifier. Another criteria to check which model is the best is by looking at the time execution (since in this challenge, that is a key factor checked as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/\n",
    "\n",
    "start = datetime.now() \n",
    "xgm = xgb_model.fit(X_features, Y) \n",
    "stop = datetime.now()\n",
    "execution_time_xgb = stop-start \n",
    "execution_time_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now() \n",
    "lgbm = lgbm_model.fit(X_features, Y) \n",
    "stop = datetime.now()\n",
    "execution_time_lgbm = stop-start \n",
    "execution_time_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now() \n",
    "gb = gb_model.fit(X_features, Y) \n",
    "stop = datetime.now()\n",
    "execution_time_gb = stop-start \n",
    "execution_time_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe ‘comparison_df’ for comparing the performance of Lightgbm and xgb. \n",
    "\n",
    "comparison_dict = {'execution time':(execution_time_lgbm, execution_time_xgb, execution_time_gb)}\n",
    "comparison_df = pd.DataFrame(comparison_dict) \n",
    "comparison_df.index= ['LightGBM','xgboost','GradientBoosting Trees'] \n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, LightGBM has the least execution time we shall proceed with this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "Before proceeding to fit our LightGB model on our datasets, we need to check for it's performance, say, if it is overfitting or underfitting the data. \n",
    "\n",
    "We shall do so by plotting the classification error of lgbm_model. This was done by referring to https://www.kaggle.com/tobikaggle/humble-lightgbm-starter-with-learning-curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate':0.03, 'n_estimators':500, 'max_depth':7, 'num_leaves':50, 'bagging_fraction':0.5,\n",
    "          'bagging_freq':2, 'feature_fraction':0.7, 'boosting_type':'gbdt',\n",
    "          'objective':'binary', 'reg_alpha':0, 'reg_lambda':0, 'random_state':10, \n",
    "          'metric':{'binary_error'}, 'verbose':0\n",
    "         }   # the best possible found via gridsearchcv\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_features, Y, test_size=0.2)\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgbm_train = lgbm.Dataset(X_train, y_train)\n",
    "lgbm_valid = lgbm.Dataset(X_valid, y_valid, reference=lgbm_train)\n",
    "\n",
    "evals_result={}\n",
    "\n",
    "model = lgbm.train(params, lgbm_train, valid_sets=[lgbm_train, lgbm_valid], \n",
    "                    evals_result= evals_result, early_stopping_rounds=100, verbose_eval=0)\n",
    "\n",
    "print('Start predicting...')\n",
    "# predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print('Plot metrics during training...')\n",
    "ax = lgbm.plot_metric(evals_result, metric='binary_error', figsize=(10,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plot feature importances...')\n",
    "\n",
    "ax = lgbm.plot_importance(model, max_num_features=15, figsize=(10,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our model is doing fine. Also we can notice that all the features are contributing towards predictions. Hence, this model will be submitted.\n",
    "\n",
    "On submitting this model, a public score of 99.40696 was evaluated. Let's see what happens when we combine all out best 3 models using a voingclassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking classifiers using maxvoting\n",
    "\n",
    "clf1 = xgb_model\n",
    "clf2 = lgbm_model\n",
    "clf3 = rf_model\n",
    "final_clf = VotingClassifier(estimators=[('xgb', clf1), ('lgbm', clf2), ('rf', clf3)], voting='hard')\n",
    "start = datetime.now() \n",
    "final_clf = final_clf.fit(X_features, Y)\n",
    "stop = datetime.now()\n",
    "execution_time = stop-start \n",
    "print('Execution_time: {}'.format(execution_time))\n",
    "print('Stacking implemented successfully!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The public score had now increased to 99.55110(rf). This seems little bit better. This would be our final model for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = final_clf.predict(X_test)\n",
    "# print(final_predictions.shape)\n",
    "# print(index.shape)\n",
    "\n",
    "results = pd.DataFrame({'INCIDENT_ID': index['INCIDENT_ID'], 'MULTIPLE_OFFENSE': final_predictions})\n",
    "results.to_csv(r'D:\\jobs\\A! Hackathon\\Novartis Challenge\\Dataset\\mysubmission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
